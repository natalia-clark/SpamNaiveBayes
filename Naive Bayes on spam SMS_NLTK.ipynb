{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56f3b7e",
   "metadata": {},
   "source": [
    "# Naive Bayes for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc3bd9-8546-4a6c-b4fe-92f5d7f89eda",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d1a15-631c-48f0-ad6c-dc1af8c32e83",
   "metadata": {},
   "source": [
    "Here we are going to use the Naive Bayes Multinomial model library to fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0165e",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ff20d-79c3-491c-aa81-5fcb9b759f82",
   "metadata": {},
   "source": [
    "We are retrieving a list of approx 5000 spam and non-spam emails (ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51adefc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message\n",
       "0      ham                      Ok lar... Joking wif u oni...\n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2      ham  U dun say so early hor... U c already then say...\n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "...    ...                                                ...\n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5567   ham               Will ü b going to esplanade fr home?\n",
       "5568   ham  Pity, * was in mood for that. So...any other s...\n",
       "5569   ham  The guy did some bitching but I acted like i'd...\n",
       "5570   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5571 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install requests\n",
    "import requests \n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "data_file = 'SMSSpamCollection'\n",
    "\n",
    "# Make request\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Get filename\n",
    "filename = url.split('/')[-1]\n",
    "\n",
    "# Download zipfile\n",
    "with open(filename, 'wb') as f:\n",
    "  f.write(resp.content)\n",
    "\n",
    "# Extract Zip\n",
    "with zipfile.ZipFile(filename, 'r') as zip:\n",
    "  zip.extractall('')\n",
    "\n",
    "# Read Dataset\n",
    "data = pd.read_table(data_file, \n",
    "                     header = 0,\n",
    "                     names = ['type', 'message']\n",
    "                     )\n",
    "\n",
    "# Show dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e930345",
   "metadata": {},
   "source": [
    "Install everything necessary: Natural Language Toolkit and the needed components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46e7d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nataliaclark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nataliaclark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK: Natural Language Toolkit\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8d218",
   "metadata": {},
   "source": [
    "## 1st step: Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf3fc8-fcc0-4e82-acae-708e7eee4ed1",
   "metadata": {},
   "source": [
    "It consists of separating the messages into words in order to be able to treat each. <BR>\n",
    "We are doing it removing punctuation at the same time (thanks to the RegexpTokenizer function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ec412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, don, t, think, he, goes, to, usf, he,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[FreeMsg, Hey, there, darling, it, s, been, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[This, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[Will, ü, b, going, to, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[Pity, was, in, mood, for, that, So, any, othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[The, guy, did, some, bitching, but, I, acted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[Rofl, Its, true, to, its, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                        [Ok, lar, Joking, wif, u, oni]  \n",
       "1     [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "2     [U, dun, say, so, early, hor, U, c, already, t...  \n",
       "3     [Nah, I, don, t, think, he, goes, to, usf, he,...  \n",
       "4     [FreeMsg, Hey, there, darling, it, s, been, 3,...  \n",
       "...                                                 ...  \n",
       "5566  [This, is, the, 2nd, time, we, have, tried, 2,...  \n",
       "5567       [Will, ü, b, going, to, esplanade, fr, home]  \n",
       "5568  [Pity, was, in, mood, for, that, So, any, othe...  \n",
       "5569  [The, guy, did, some, bitching, but, I, acted,...  \n",
       "5570                   [Rofl, Its, true, to, its, name]  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize: create words from sentences, and removes punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #w+ means one or more consecutive word characters\n",
    "data['tokens'] = data.apply(lambda x: tokenizer.tokenize(x['message']), axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508d4df",
   "metadata": {},
   "source": [
    "## 2nd step: Elimination of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559b5ea-c2f3-4aa6-b39c-32e42f8bfa21",
   "metadata": {},
   "source": [
    "Elimination of words that normally do not add value (prepositions, conjunctions, etc.), only noise (there are exceptions, though) <BR> This is done to reduce size of our dataset (already large). \n",
    "<BR> There are many sources of stopwords in Python, for English language, Spacy has 326 words, Gensim 337, Scikit-learn 318, NLTK library, 179. \n",
    "<BR> More or less is not better in itself, you need to read them and assess whether you want to include or remove some (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3b9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'mostly', 'could', 'with', 'hereafter', 'further', 'be', 'sincere', 'such', 'formerly', 'either', 'mine', 'sixty', 'why', 'whether', 'since', 'anywhere', 'mill', 'must', 'have', 'cry', 'nothing', 'beyond', 'empty', 'twelve', 'give', 'up', 'not', 'here', 'becomes', 'onto', 'third', 'had', 'also', 'beforehand', 'yourselves', 'cant', 'un', 'from', 'around', 'am', 'other', 'in', 'among', 'these', 'toward', 'elsewhere', 'thence', 'behind', 'each', 'being', 'per', 'always', 'next', 'several', 'eg', 'ever', 'it', 'someone', 'again', 'however', 'own', 'thru', 'back', 'because', 'latterly', 'seemed', 'into', 'hence', 'who', 'they', 'four', 'many', 'while', 'whose', 'to', 'front', 'therefore', 'she', 'throughout', 'become', 'will', 'an', 'due', 'anyhow', 'nobody', 'move', 'bill', 'made', 'towards', 'their', 'before', 'bottom', 'at', 'another', 'over', 'couldnt', 'full', 'almost', 'part', 'hers', 'across', 'whatever', 'fill', 'between', 'nevertheless', 'same', 'can', 'ours', 'those', 'only', 'that', 'no', 'top', 'yourself', 'six', 'three', 'was', 'may', 'how', 'everywhere', 'perhaps', 'thereby', 'whereupon', 'through', 'do', 'hereupon', 'less', 'still', 'ourselves', 'the', 'get', 'both', 'seems', 'by', 'enough', 'which', 'me', 'too', 'anything', 'somehow', 'cannot', 'one', 'against', 'him', 'found', 'yet', 'see', 'namely', 'ltd', 'moreover', 'out', 'were', 'detail', 'hereby', 'wherein', 'us', 'whereafter', 'few', 'call', 'afterwards', 'fifteen', 'every', 'none', 'itself', 'together', 'you', 'twenty', 'if', 'where', 'besides', 'above', 'noone', 'when', 'off', 'indeed', 'them', 'whereby', 'somewhere', 'five', 'fire', 'alone', 'myself', 'show', 'therein', 'go', 'sometime', 'thereafter', 'so', 'he', 'some', 'became', 'describe', 'first', 'fifty', 'as', 'keep', 'nowhere', 'under', 'are', 'con', 'most', 'rather', 'about', 'its', 'our', 'below', 'last', 'serious', 'never', 'system', 'well', 'former', 'than', 'would', 'wherever', 'side', 'anyway', 'thus', 'your', 'all', 'whenever', 'along', 'forty', 'done', 'neither', 'is', 'whom', 'inc', 'two', 'everyone', 'my', 'has', 'everything', 'until', 'interest', 'hasnt', 'then', 'without', 'ten', 'except', 'find', 'now', 'etc', 'nor', 'might', 'often', 'seem', 'themselves', 'we', 'name', 'on', 'else', 'down', 'de', 'her', 'put', 'thick', 'something', 'any', 'during', 'anyone', 'herein', 'beside', 'very', 'whither', 'amoungst', 'amount', 'what', 'via', 'and', 'this', 'more', 'least', 'thin', 'his', 'himself', 'seeming', 'for', 'nine', 'whence', 'although', 'thereupon', 'whereas', 'whoever', 'otherwise', 'whole', 'eight', 're', 'amongst', 'there', 'please', 'within', 'already', 'upon', 'or', 'even', 'after', 'meanwhile', 'ie', 'though', 'sometimes', 'yours', 'much', 'others', 'should', 'but', 'a', 'eleven', 'co', 'take', 'once', 'becoming', 'of', 'i', 'hundred', 'herself', 'latter', 'been'})\n"
     ]
    }
   ],
   "source": [
    "# Sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "print(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a495b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0df5b6-4b43-4940-865f-2fdbd5988a09",
   "metadata": {},
   "source": [
    "We will be using the stopwords in nltk library\n",
    "- need to decide which list fits the requirements better\n",
    "- sometimes disagree and build own list if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9d03f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[FreeMsg, Hey, darling, 3, week, word, back, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[This, 2nd, time, tried, 2, contact, u, U, 750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[Will, ü, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[Pity, mood, So, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[The, guy, bitching, I, acted, like, intereste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[Rofl, Its, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                        [Ok, lar, Joking, wif, u, oni]  \n",
       "1     [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "2         [U, dun, say, early, hor, U, c, already, say]  \n",
       "3     [Nah, I, think, goes, usf, lives, around, though]  \n",
       "4     [FreeMsg, Hey, darling, 3, week, word, back, I...  \n",
       "...                                                 ...  \n",
       "5566  [This, 2nd, time, tried, 2, contact, u, U, 750...  \n",
       "5567           [Will, ü, b, going, esplanade, fr, home]  \n",
       "5568                      [Pity, mood, So, suggestions]  \n",
       "5569  [The, guy, bitching, I, acted, like, intereste...  \n",
       "5570                            [Rofl, Its, true, name]  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "# using nltk - shorted list of stopwords, keeping if not in stopwords\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ceab0",
   "metadata": {},
   "source": [
    "## 3rd step: Apply stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c461a3-e26e-4799-888e-3a5baf55c7dd",
   "metadata": {},
   "source": [
    "**Stemming**: Stemming is a process of reducing inflected words to their word stem, base or root form. This is done by removing affixes, which are word parts that are added to the root to change its meaning or grammatical function. For example, the word \"running\" can be stemmed to \"run\" by removing the suffix \"-ing\".\n",
    "\n",
    "**Lemmatization**: It is more used in NLP projects in English. It involves taking into account the morphology and grammar of the words, it would convert all the words to their base, in such a way that they come to mean the same thing. Example: For the words \"am\", \"are\", \"is\", stemming would not work, therefore they would all be lemmatised to \"be\". \n",
    "\n",
    "#####  Want to apply both because stemming can cut the word down to its stem, and lemmatization can infer some meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dd891-e916-40e8-911d-1f0924c71734",
   "metadata": {},
   "source": [
    "First, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe38b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Porter stemming\n",
    "stemmer = PorterStemmer()\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [stemmer.stem(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d8bbb2-b39d-46d9-a04c-6da4319acd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, darl, 3, week, word, back, i, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, they, treat, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>[as, per, request, mell, mell, oru, minnaminun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>[winner, as, valu, network, custom, select, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>[had, mobil, 11, month, u, r, entitl, updat, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>[i, gonna, home, soon, want, talk, stuff, anym...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message  \\\n",
       "0   ham                      Ok lar... Joking wif u oni...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  U dun say so early hor... U c already then say...   \n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4  spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "5   ham  Even my brother is not like to speak with me. ...   \n",
       "6   ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "7  spam  WINNER!! As a valued network customer you have...   \n",
       "8  spam  Had your mobile 11 months or more? U R entitle...   \n",
       "9   ham  I'm gonna be home soon and i don't want to tal...   \n",
       "\n",
       "                                              tokens  \n",
       "0                       [ok, lar, joke, wif, u, oni]  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "2      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "3    [nah, i, think, goe, usf, live, around, though]  \n",
       "4  [freemsg, hey, darl, 3, week, word, back, i, l...  \n",
       "5  [even, brother, like, speak, they, treat, like...  \n",
       "6  [as, per, request, mell, mell, oru, minnaminun...  \n",
       "7  [winner, as, valu, network, custom, select, re...  \n",
       "8  [had, mobil, 11, month, u, r, entitl, updat, l...  \n",
       "9  [i, gonna, home, soon, want, talk, stuff, anym...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e647811-3198-4e63-914f-6debd099b947",
   "metadata": {},
   "source": [
    "Now, we apply lemmatization. We download a pre-trained spacy model called en_core_web_sm, which is a NLP model for the English language. <BR>It includes information about lemmatisation, morphological analysis, grammatical tagging and other tasks related to NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66bbc6d5-57f4-428b-b5e4-feefb8a48457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a1f87a3-2d30-43b0-9ec2-ebc55fbe30c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, I, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, darl, 3, week, word, back, I, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[thi, 2nd, time, tri, 2, contact, u, u, 750, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>[will, ü, b, go, esplanad, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[piti, mood, so, suggest]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[the, guy, bitch, I, act, like, interest, buy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[rofl, it, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                          [ok, lar, joke, wif, u, oni]  \n",
       "1     [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "2         [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "3       [nah, I, think, goe, usf, live, around, though]  \n",
       "4     [freemsg, hey, darl, 3, week, word, back, I, l...  \n",
       "...                                                 ...  \n",
       "5566  [thi, 2nd, time, tri, 2, contact, u, u, 750, p...  \n",
       "5567               [will, ü, b, go, esplanad, fr, home]  \n",
       "5568                          [piti, mood, so, suggest]  \n",
       "5569  [the, guy, bitch, I, act, like, interest, buy,...  \n",
       "5570                             [rofl, it, true, name]  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Aplicar la lematización a la columna 'tokens'\n",
    "data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee5f86-97c3-4e3b-b956-538054083d5b",
   "metadata": {},
   "source": [
    "And we can optionally export cleaned data before applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d51d0b0-a2a3-4614-86a3-9df7bdc3ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabd271-8a15-44aa-b7f6-3e45b45904b8",
   "metadata": {},
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd1a7b",
   "metadata": {},
   "source": [
    "Naive Bayes model admits two types of source data:<BR>\n",
    "\n",
    "**1. A Matrix** that shows, for each document, either how many times each of the words in all the documents has appeared or how important is the word in each document compared to the whole source of data. We will run an example first with a Matrix showing frequencies and the 2nd with the more complex one. <BR>\n",
    "**2. An array of appearances**. It is similar to a TF matrix, but in this case, instead of indicating the number of occurrences, it simply indicates whether or not that word appeared.<BR>\n",
    "<BR>If we have plenty of data, normally a Matrix is preferred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12c21",
   "metadata": {},
   "source": [
    "First, we are detokenizing the rows, that is, joining back the words, once they have been cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40f2e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah I think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darl 3 week word back I like fun s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>thi 2nd time tri 2 contact u u 750 pound prize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>will ü b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>piti mood so suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>the guy bitch I act like interest buy someth e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl it true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "0      ham                      Ok lar... Joking wif u oni...   \n",
       "1     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2      ham  U dun say so early hor... U c already then say...   \n",
       "3      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "4     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "...    ...                                                ...   \n",
       "5566  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5567   ham               Will ü b going to esplanade fr home?   \n",
       "5568   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5569   ham  The guy did some bitching but I acted like i'd...   \n",
       "5570   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                                 tokens  \n",
       "0                                 ok lar joke wif u oni  \n",
       "1     free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "2                   u dun say earli hor u c alreadi say  \n",
       "3                nah I think goe usf live around though  \n",
       "4     freemsg hey darl 3 week word back I like fun s...  \n",
       "...                                                 ...  \n",
       "5566  thi 2nd time tri 2 contact u u 750 pound prize...  \n",
       "5567                       will ü b go esplanad fr home  \n",
       "5568                               piti mood so suggest  \n",
       "5569  the guy bitch I act like interest buy someth e...  \n",
       "5570                                  rofl it true name  \n",
       "\n",
       "[5571 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unify the strings once again\n",
    "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59061ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3ab3e",
   "metadata": {},
   "source": [
    "Now we are splitting our data as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e488317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Make split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data['tokens'], \n",
    "    data['type'], \n",
    "    test_size= 0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681c3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size:  4456\n",
      "Testing data set size:  1115\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data set size: \", len(x_train))\n",
    "print(\"Testing data set size: \", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad2d69",
   "metadata": {},
   "source": [
    "And here is where we create the matrix. The CountVectorizer convert a collection of text documents into a matrix of term or word counts. <BR>Each document is represented as a vector where each element of the vector corresponds to the frequency count of a specific word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9c5943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1115x6432 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9786 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents = 'ascii', \n",
    "    lowercase = True\n",
    "    )\n",
    "\n",
    "# Fit vectorizer & transform it\n",
    "vectorizer_fit = vectorizer.fit(x_train)\n",
    "x_train_transformed = vectorizer_fit.transform(x_train)\n",
    "x_test_transformed = vectorizer_fit.transform(x_test)\n",
    "\n",
    "x_test_transformed\n",
    "# sparse matrix because many empty cells if remembering the matrix\n",
    "# most powerful way to fit a multinomial naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc71599-97a2-4b0f-bf27-be54ffe41f95",
   "metadata": {},
   "source": [
    "### Multinomial NB fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f099b",
   "metadata": {},
   "source": [
    "Now we build the Naive Bayes model and train it. \n",
    "<BR> There are two NB models in sklearn we can use: <BR>\n",
    "- Gaussian. It is useful when we have metric features\n",
    "- Multinomial. Appropriated for text classification where variables represent frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139af65-7441-44d2-9e9c-68a359f6220e",
   "metadata": {},
   "source": [
    "We will be applying Multinomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2846dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train the model\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes_fit = naive_bayes.fit(x_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f12113",
   "metadata": {},
   "source": [
    "And we make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d41a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Make predictions\n",
    "train_predict = naive_bayes_fit.predict(x_train_transformed)\n",
    "test_predict = naive_bayes_fit.predict(x_test_transformed)\n",
    "\n",
    "def get_scores(y_real, predict):\n",
    "  ba_train = balanced_accuracy_score(y_real, predict)\n",
    "  return ba_train\n",
    "\n",
    "train_scores = get_scores(y_train, train_predict)\n",
    "test_scores = get_scores(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600829ba",
   "metadata": {},
   "source": [
    "This is the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71a12d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Train Accuracy\n",
      "Accuracy of Train set = 98.23%\n",
      "\n",
      "## Test Accuracy\n",
      "Accuracy of Test set = 96.19%\n"
     ]
    }
   ],
   "source": [
    "print(\"## Train Accuracy\")\n",
    "print(\"Accuracy of Train set = {0:.2%}\".format(train_scores))\n",
    "print(\"\\n## Test Accuracy\")\n",
    "print(\"Accuracy of Test set = {0:.2%}\".format(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03e905",
   "metadata": {},
   "source": [
    "And the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b937c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat = confusion_matrix(y_test, test_predict)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=naive_bayes_fit.classes_, yticklabels=naive_bayes_fit.classes_)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
